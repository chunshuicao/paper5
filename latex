\documentclass[10pt,journal,compsoc]{IEEEtran}
%\documentclass[9.5pt,journal,final,finalsubmission,twocolumn]{IEEEtran}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{subfigure}
\usepackage{float}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{rotating}
%\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{bbm}

\usepackage{cite}

\usepackage{setspace}


\usepackage{lipsum}
\usepackage[parfill]{parskip}
\usepackage{tabularx, calc}

\usepackage[numbers,sort&compress]{natbib}

%\usepackage{algorithm2e}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\hyphenation{optical net-works semi-conductor}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{multirow}
\usepackage{indentfirst}
\setlength{\parindent}{1em}

\newfloat{procedure}{htbp}{loa}
\floatname{procedure}{Procedure}

\begin{document}



\title{Feedback Convolutional Neural Networks for Visual Localization and Segmentation}

\author{Chunshui~Cao, Yongzhen~Huang,~\IEEEmembership{Member,~IEEE,} Yi~Yang, Liang~Wang,~\IEEEmembership{Senior Member,~IEEE,} and Tieniu~Tan,~\IEEEmembership{Fellow,~IEEE,}

\thanks{Chunshui Cao is with University of Science and Technology of China. Email: ccs@mail.ustc.edu.cn.
Yongzhen Huang, Liang Wang and Tieniu Tan are with National Laboratory of Pattern Recognition, CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences. Email: \{yzhuang, wangliang, tnt\}@nlpr.ia.ac.cn.
Yi Yang is with Baidu Research. Email: yangyi05@baidu.com.}
}
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, November~2016}
%{Cao \MakeLowercase{\textit{et al.}}: Feedback Neural Networks}
\markboth{December~2016}
{Cao \MakeLowercase{\textit{et al.}}: Feedback Neural Networks}
\IEEEtitleabstractindextext{
\begin{abstract}
Feedback, as a kind of very common and important mechanism in the human visual system, has not attracted sufficient attention in designing pattern recognition algorithms.
In this paper, we propose that feedback plays a critical role for in-depth analysis of convolutional neural networks (CNN), e.g., how a neuron in CNN represents a part of an object, and how a set of local neurons form global perception of an object.
To model the feedback in CNN, we develop a novel method consisting of two main operations, i.e., selective neural pathway pruning and pattern recovering, with mathematical analysis and strict proofs provided.
We call the proposed model as feedback convolutional neural network (Feedback CNN), with which we can explicitly describe what a neuron represents with respect to a specific object category.
The Feedback CNN model is easily trained with only category labels, but has the ability to localize and segment objects (like human learning), indicating that a classification model is capable of learning the essence of visual objects via well-designed feedback.
The analysis of object/part visualization and relevant neuron selection reveals the potentially close relationship between neurons in Feedback CNN and parts of objects.
Moreover, we design two quantitative experiments in terms of weakly supervised object localization and segmentation, respectively, and the experimental results on ImageNet and Pascal VOC show that our method largely outperforms the state-of-the-art ones.
\end{abstract}
\vspace{0mm}
\begin{IEEEkeywords}
Feedback, neural pathway pruning, pattern recovering, weakly supervised localization, weakly supervised segmentation.
\end{IEEEkeywords}}
\vspace{0mm}
\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle
\section{Introduction}
In recent years, convolutional neural networks (CNN) have made tremendous progress in a variety of vision tasks, including object classification~\cite{alexnet,googlenet,vgg,resnet}, localization~\cite{rcnn,ssd}, and semantic segmentation~\cite{segFCN,segCRFasRNN}.
These tasks are usually treated as different problems and handled separately with different frameworks.
To localize and segment objects in images, most popular approaches~\cite{rcnn,ssd,segFCN,segCRFasRNN} need strong supervision information for training, e.g., bounding boxes or segmentation masks.
However, collecting a large amount of such data is often expensive and time-consuming.
Therefore, it is ideal to localize and segment objects only when category labels are available, and this manner is naturally more like human learning.

Recently, the studies~\cite{zeiler,zhou2014object} have shown that the convolutional units in CNNs only trained for the classification task have the potential to learn a part of semantic patterns like object parts or even the whole object.
%In the training phase of these CNNs, these methods, category information is implicitly used as a kind of feedback signal in training.
In the training phase of these CNNs, category information is implicitly used as a kind of feedback signal.
In neurobiology, the Biased Competition Theory~\cite{desimone1995neural} claims that when searching for objects in a scene, the human visual cortex is enhanced by the top-down feedback after processing visual information in a bottom-up manner, and irrelevant neurons will be suppressed in feedback loops, thus leading to the selectivity in neuron activations.
This top-down feedback mechanism is involved in a variety of visual perception tasks, including selective attention, scene segmentation, and the encoding and recall of learned information~\cite{gilbert2013top}.

\begin{figure}[!tp]
\centering
\vspace{0mm}
\includegraphics[width = 0.9 \linewidth]{figure/model/pipeline.pdf}
\caption{Feedback CNN. Given an input image, we perform a normal feed-forward to predict the class label and set it as the target. Then use the pruning operation to select related neurons, and perform the recovering operation on these selected neurons to obtain target-relevant visualization and energy maps. Each selected neuron is highly related to the object parts, which is shown by visualizing the selected neurons respectively.}
\label{fig:pipeline}
\end{figure}
\begin{figure*}[!tp]
\vspace{0mm}
\centering
\includegraphics[width = 1.0 \linewidth]{figure/introduction/simple_pipeline/sp.pdf}
\caption{A simple pipeline for object localization and segmentation via the proposed Feedback CNN. Given an input image, we utilize Feedback CNN to generate target-relevant visualization and energy maps. Based on these maps, objects can be easily localized and segmented from the input image. Best viewed in color.}
%we selectively prune the networks by back-propagating top-down information. After that, we can get target-relevant regions in visualization and energy maps, based on which object localization and segmentation can be easily achieved. Best viewed in color.}
\label{fig:sp}
\end{figure*}
Inspired by the aforementioned observations, we propose a Feedback CNN to implement the unified processing of object recognition, localization and semantic segmentation.
As shown in Fig.~\ref{fig:pipeline}, we first acquire target-relevant neurons by selectively pruning the neural pathway according to the predicted class label, and then simultaneously recover the activated patterns learned by the selected neurons in the image space.
In this way, the object of interest can be effectively captured in visualization and energy maps.

Specifically, we introduce {\bf latent gate variables} to control the hidden neurons and formulate feedback as an optimization problem that is solved by our newly developed algorithms i.e., Feedback Selective Pruning (FSP) and Feedback Recovering (FR).
FSP selects target-relevant neurons in hidden layers via back propagation pathways.
FR restores visual information that falls in the receptive field of a given neuron.
Consequently, combined with the advantages of both FSP and FR, the proposed Feedback CNN can effectively produce task-specific visualization and energy maps with high quality.
In particular, we visualize several neurons selected by FSP in Fig.~\ref{fig:pipeline} via applying FR separately.
The visualization results indicate that the selected neurons are highly relevant to the target object and correspond to different parts of the object.
As a consequence, Feedback CNN selects target-relevant and suppresses irrelevant neurons during the top-down inference, guiding the model to focus on the most salient image regions that are highly related to the given category label.

Accordingly, Feedback CNN can reinforce a normal CNN trained for object classification to localize and segment interested objects from natural images, as illustrated in Fig.~\ref{fig:sp}.
More specifically, a CNN classifier takes an image in Fig.~\ref{fig:sp}(a) as an input, then perform traditional feed-forward inference, as shown in Fig.~\ref{fig:sp}(b).
After that, the inferred category, e.g., ``Train" for the first image, is set as the target for our feedback model in Fig.~\ref{fig:sp}(c), and thus only the neurons related to ``Train" will be activated.
As a result, in Fig.~\ref{fig:sp}(d)(e), only the salient regions truly related with ``Train" are captured both in visualization and energy maps.
With the help of these maps, it is easy to localize and even segment objects without any strongly supervised model learning, as shown in Fig.~\ref{fig:sp}(f) and Fig.~\ref{fig:sp}(g), respectively.
As a consequence, Feedback CNN provides an innovative way to integrate object recognition, localization and segmentation into a unified framework, which is more like human learning.

%This work makes a big progress along our previous one~\cite{cao2015look}.
A preliminary version of the work was reported in~\cite{cao2015look}.
Compared with~\cite{cao2015look}, apart from more comprehensive description, analysis and experiments, this paper formulates two completely new algorithms with strict mathematical proofs for modeling feedback, which exhibits much stronger capability for task-specific neuron selection and object capturing.
Based on the proposed new model, we can obtain energy maps with high signal-to-noise ratio, complete target description and clear object boundaries, which well paves the way for weakly supervised object localization and segmentation.
In contrast, the model discussed in~\cite{cao2015look} suffers from noise and is confined to weakly supervised object localization only.

%Most notably, the optimization problem in ~\cite{cao2015look} is further analyzed and the algorithms proposed in this paper are completely new, which are much more powerful for modeling feedback.
%Particularly, a new approach for weakly supervised object localization and segmentation is also developed.
%By a plenty of quantitative and qualitative analysis, we demonstrate that with these improvements the new Feedback CNN exhibits much stronger capability for task-specific neuron selection and object capturing, which greatly improves the application value of Feedback CNN.
The main contributions of this paper are summarized as follows:
1) We develop two new algorithms (i.e., FSP and FR mentioned above) to model feedback mechanism in CNN as two effective solutions of the defined optimization problem with elegant mathematic proofs.
2) Via the visualization and energy maps, we demonstrate that the proposed Feedback CNN has the ability to select useful neurons related to expected objects or its parts.
3) We apply Feedback CNN for weakly supervised object localization and segmentation, respectively.
The big improvement over the state-of-the-art methods verifies the value of Feedback CNN.

\section{Related Work}

In this work, we will demonstrate that by selecting target-relevant hidden neurons and restoring the learned patterns of those selected neurons in the image space, a deep CNN merely trained for recognition can be generalized to localize and segment objects from images.
Some studies closely related to this work are briefly discussed as follows.

\subsection{Deep CNN}
Recent years have witnessed the great success of deep neural networks in various computer vision tasks ~\cite{alexnet,googlenet,vgg,resnet,rcnn,ssd,segFCN,segCRFasRNN}.
Particularly, deep CNN has achieved human level performance for object recognition~\cite{alexnet, googlenet, vgg, resnet, salakhutdinov2009deep, bengio2013representation}.
It learns features and classifiers simultaneously from a large scale of training samples.
The discriminative ability of deep CNN is greatly improved after a number of approaches being proposed, including Dropout~\cite{srivastava2014dropout}, PReLU~\cite{he2015delving}, and Batch Normalization~\cite{ioffe2015batch}.
Moreover, there are considerable interests in enhancing deep CNN with greater capacity, in which the networks are generally designed to be deeper or wider~\cite{googlenet,resnet,wide}.
The detailed analysis of CNN for the classification task~\cite{zhou2014object,zeiler} shows that semantic patterns can be learned from the given training data.

All of these advances pave a way for constructing a feedback model in CNN.
By introducing the feedback mechanism in those models, it is expected that we are capable of performing object localization and semantic segmentation easily under weakly supervised conditions.

\subsection{Top-down Feedback}
Top-down Feedback plays an important role in human vision system, e.g., objects localization and segmentation from complex backgrounds~\cite{gilbert2013top}, feature grouping~\cite{gilbert2007brain}, perceptual filling~\cite{bar2006top}, and tuning neurons' receptive fields and functional roles~\cite{gilbert2013top}.
Recently, some efforts have been made to embed feedback mechanism into deep neural networks.
The convolutional latent variable models (CLVMs) in ~\cite{hu2015bottom} model feedback by treating units as latent variables in a global energy function.
An earlier study is presented in Deep Boltzmmann Machines (DBM) for feature selection~\cite{wang2014attentional}.
Meanwhile, Recurrent Neural Networks (RNN) and Long Short Term Memory (LSTM)~\cite{hochreiter1997long} are utilized to capture attention drifting in a dynamic environment and learn the feedback mechanism via reinforcement learning~\cite{stollenga2014deep,mnih2014recurrent}.
On the other hand, Deep Boltzmmann Machines (DBM)~\cite{salakhutdinov2009deep,sohn2013learning} and Deconvolutional Neural Networks~\cite{zeiler} attempt to formulate feedback as a reconstruction process at the training stage.

In this paper, we propose to formulate feedback as an optimization problem for neuron selection.
The main difference is that the proposed feedback is involved to selectively modulate the status of hidden neurons at the testing stage, and thus it does not affect the model training which is adopted in most previous studies introduced above.
In particular, the feedback mechanism is embedded into a deep classification model, wherein feed-forward connections serve as information carriers, and useful information for a particular goal is selected by top-down feedback.

\subsection{Weakly-supervised Object Localization and Segmentation}
The direct applications of the proposed Feedback CNN are weakly supervised object localization and segmentation, and thus in this subsection we introduce some representatives in these two fields.

Many studies are developed for weakly supervised object localization using CNN~\cite{deepinside,sohn2013learning,oquab2015object,bergamo2014self,zhou2015learning,zhang2016top}.
A technique for self-teaching object localization is proposed in~\cite{bergamo2014self}.
The approaches proposed in~\cite{sohn2013learning,oquab2015object} use global average pooling and max pooling to generate class-specific energy maps, and localize objects based on these maps.
The work of~\cite{deepinside} localizes objects by segmenting objects in an image based on the noisy energy map generated by class specified gradients.
The approach in~\cite{zhou2015learning} needs re-training the recognition model with the average pooling layer.
The method in~\cite{zhang2016top} is based on a probabilistic ``winner-take-all" process, in which activation values and positive convolutional weights are involved in the calculation of Marginal Winning Probability.
The energy maps generated by~\cite{zhou2015learning} and~\cite{zhang2016top} mainly highlight the most discriminative parts of objects but lose objects boundaries and suffer from noise and interference.

Recently, some remarkable approaches have been proposed for weakly-supervised semantic segmentation~\cite{bearman2015s,kim2016scale,ccnn,segcvpr15,papandreou2015weakly}.
The approaches presented in~\cite{bearman2015s} and~\cite{segcvpr15} adopt different pooling strategies to train deep networks from the viewpoint of multiple instance learning.
Both CCNN~\cite{ccnn} and EM-Adapt~\cite{papandreou2015weakly} develop a self-training framework and enforce the consistency between the per-image annotation and the predicted segmentation masks by different constraints.

Different from these methods, the proposed Feedback CNN in this paper can simultaneously perform object recognition, localization and semantic segmentation.
Under the same weakly supervised settings, we merely need to train a classification model and then perform object localization and semantic segmentation without any strongly supervised model learning.

\section{Feedback CNN}

\subsection{Problem Definition and Formulation}
\subsubsection{Problem Description}
%It is generally known that object recognition, localization and segmentation are strongly correlated to each other.
%In this section, we demonstrate that it is technically feasible to integrate these three tasks into one framework.
For easy understanding, we use a figure to explain the meaning of the main variables in the definition.
\begin{figure}[!tp]
\centering
\includegraphics[width = 0.85 \linewidth]{figure/introduction/idea/idea.pdf}
\caption{Pathway pruning. (a) Neural pathways between two neurons. (b) The neural pathways are abstracted as a CP. (c) All CPs between input pixels and a target neuron. (d) Selectively prune the CPs. (e) The preserved target-related CPs. (f) Object localization and segmentation by the preserved CPs.}
\label{fig:idea} %% label for entire figure
\end{figure}

As shown in Fig.~\ref{fig:idea}(a)(b), every pixel in the input image initiates several neural pathways to a high-level semantic neuron $F$ which represents a face, and all these pathways can be abstracted as a connecting pathway (CP) between a pixel and $F$.
Consequently, all pixels of the input image will be connected to $F$ by their own CPs, as shown in Fig.~\ref{fig:idea}(c).
Let $P$ denote the set of all these CPs.
And all visual information of the face and the background is transmitted to the neuron $F$ in a bottom-up manner.
Let $R$ be a rule to judge whether a CP links a target object pixel to the target neuron $F$ or not.
Then the set $P$ can be divided into two subsets $T$ and $B$ according to $R$, as described in the following definition:

\noindent \emph{{\bf Problem definition:} Let P = \{CPs from all pixels to $F$\}, T = \{CPs from the target object to $F$\} and B = \{CPs from the background to $F$\}.
The problem is to find a rule $R$, s.t. $P=T\bigcup B \ and \ T\bigcap B=\emptyset$.
}

If we find the rule $R$ and wipe out all the connections in $B$, as demonstrated in Fig.~\ref{fig:idea}(d) and (e), the target object can be localized and segmented in a top-down manner, which explains the technical feasibility of integrating object recognition, localization and segmentation together.
To conclude, a strategy is required to selectively modulate the bottom-up pathways in neural networks according to the target semantic neuron.
%height=0.7\linewidth

In this paper, feedback serves as the rule R for the neural pathway selection.
Specifically, we introduce latent gate variables and build feedback connections to control the hidden neurons, as demonstrated in Fig.~\ref{fig:feedback}.
Neural pathways pruning is accordingly transferred to neuron selection.
Fig.~\ref{fig:feedback}(c) demonstrates that feedback connections are established from top ``goal" neurons to all the additional gates.
In addition to the bottom-up inference in traditional convolutional neural networks, Feedback CNN infers about the status of all the additional gates according to the ``goal" of the network in several feedback loops.
After the feedback selection, irrelevant hidden neurons will be suppressed which means that lots of non-target connecting pathways are clipped.
Finally, we can obtain the information of objects in a top-down propagation.

\subsubsection{Problem Formulation}
CNNs usually consist of several stacked feed-forward layers, including the convolutional layer, the rectified linear units (ReLU) layer, the max pooling layer and some other nonlinear layers like the soft-max layer and the sigmoid layer.
The ReLU and the max pooling layers can be interpreted as ``gates" attached to the output of convolutional layers and the status of these ``gates" are actually controlled by the input information.
The network expresses patterns during the feed-forward phase in a bottom-up manner.
For each output neuron in the convolutional layer, as long as the pattern lying on its receptive field is matched with its learned pattern and the similarity is stronger than its neighbors, the corresponding ``gates" in the subsequent ReLU and max pooling layers will be opened.
When targeting at a particular semantic label, these activated neurons could be either helpful or harmful, and they potentially involve too much noise and irrelevant information.
So it is reasonable to put additional gates on the output of the ReLU and max pooling layers to further turn off those activated but irrelevant neurons.
In particular, these new gates are determined by both bottom-up feedforward information and top-down feedback information.

Mathematically, we formulate such feedback mechanism as an optimization problem by introducing additional gate variables $Z$.
Given an image $I$ and a CNN with parameters $W$, we denote the score of the target neuron as $S$ and the mapping function from $I$ to $S$ as $f(I)$.
As mentioned above, the convolutional units are already controlled by ReLU and max pooling gates, thus we put additional gates $Z$ on the top of each ReLU neuron and max pooling neuron.

\begin{figure}
\vspace{0mm}
\centering
\includegraphics[width = 0.9 \linewidth,height=0.45\linewidth]{figure/introduction/feedback/feedback_b.pdf}
\caption{Building the feedback connection. (a) An original CNN. (b) Put additional gates on the hidden neurons.
(c) Build feedback connections from the target neuron to all the additional gates.}
\label{fig:feedback} %% label for entire figure
\end{figure}

Denote $z_{ijc}^l$ as the additional gate for the neuron at location $(i, j)$ and channel $c$ in layer $l$, and denote the mapping function as $f(I,Z)$.
The optimization problem is formulated as:
\begin{equation}
\begin{aligned}
&\max\limits_{Z}S=f(I,Z)\\
&s.t.\ \  z_{ijc}^l\in{\{0,1\}}\ \forall \ {l,i,j,c}\\
&type(l)\ = {\ ReLU \ or \ max \ pooling}
\end{aligned}
\end{equation}

This leads to an integer programming problem, which is NP-hard with the current deep net architecture.
Fortunately, locally optimal solutions can be derived for this problem when $f(I,Z)$ is linearly approximated.

\subsubsection{Linear Approximation}
It is well known that CNN is a nonlinear mapping function, since it has some nonlinear layers such as the ReLU layer, the max pooling layer and so on.
However, given an input image $I_0$, we can approximate the nonlinear CNN around $I_0$ in the input space with a linear function, denoted as $F(I_0)$.
Specifically, for the neurons in ReLU and max pooling, we fix their status after the feed-forward procedure.
And other neurons in the nonlinear mapping functions, e.g., $g(x)$, are approximated with their first order Taylor Expansion at $x_0$ which is a specific input determined by $I_0$, as described in Equation (2).
Furthermore, since the max pooling layers always appear after the ReLU layers (like gating), we initialize their additional gates with their status after the first feed-forward and do not change them any more.
That means the algorithm will concentrate on the additional gates for ReLU neurons.
\begin{equation}
\begin{aligned}
&g(x)=g(x_0 )+g'(x_0)(x-x_0 )+o(x-x_0)\\
&g(x) \approx g(x_0 )+g'(x_0)(x-x_0)
\end{aligned}
\end{equation}

After the above approximation operations, the target $S$ can be described based on an ReLU layer:
\begin{equation}
\begin{aligned}
&S=F(I_0,Z)=\begin{matrix}\sum_{ijc}^l\alpha_{ijc}^lz_{ijc}^lx_{ijc}^l \end{matrix} \\
&\alpha_{ijc}^l=\frac{\partial S}{\partial (z_{ijc}^lx_{ijc}^{l})}\\
&type(l)= {ReLU}
\end{aligned}
\end{equation}
where $x_{ijc}^l$ is the neuron at $(i, j)$ of channel $c$ in layer $l$.
For convenience, layer $l$ means the ReLU layer $l$ in the rest of this paper.
Note that we denote all the ReLU layers from bottom to top with index $l =1,2,\ldots,N$.
And $\alpha_{ijc}^l$ can be calculated as the sum of weights of all neuron pathways from $x_{ijc}^l$ to the target neuron $S$.
Hence, we call $\alpha_{ijc}^l$ as the Summation Weight of pathways (SW).
Then, the feedback optimization problem is transited as:
\begin{equation}
\begin{aligned}
&\max\limits_{Z}S=F(I_{0},Z)\\
&s.t.\ \  z_{ijc}^l\in{\{0,1\}}\ \forall \ {l,i,j,c}\\
&type(l)\ =ReLU
\end{aligned}
\end{equation}
Next, we present two solutions, and both of them are layer-by-layer optimizing procedure.

\subsection{Solutions}
\subsubsection{Feedback Recovering}
Due to the hierarchical network architecture of CNN, the target $S$ in Equation~(4) can be expanded via nested functions from top to bottom layer.
Consequently, to maximize $S$, we can optimize additional gates $Z$ layer-by-layer in a top-down order, as demonstrated in Algorithm~1.
Note that we denote the mapping function of the target $S$  after updating the ReLU layer $l$ as $S_{l}$, and use subscript $k$ to replace $i,j,c$ for simplicity.
$w_{k'}^{l-1}$ is the weight between $x_{k'}^{l-1}$  and $x_{k}^l$ when the convolution operation is performed from layer $l-1$ to layer $l$.
A sign function $\delta(x)$ is described as:
\begin{equation}
       \delta(x)=
        \begin{cases}
        1 &\mbox{if $x > 0$}\\
        0 &\mbox{if $x \leq 0$}
       \end{cases}
\end{equation}


\begin{algorithm}
\renewcommand\baselinestretch{1.5}\selectfont
  \small
  \small
  \caption{Feedback Recovering Algorithm}
 \begin{algorithmic}[]
   \State $\textbf{INPUT}:\ image\ I_{0}$, $target \ neuron\ with\ score\ function\ S$
   \State $\textbf{DO}:$
   \State $Initialize\ all\ Z\ with\ 1$
    \For {$iteration=1\ to\ max\ iteration$}
      \State $feedforward$
      \If{$iteration==1$}
      \State $Do \ linear\ approximation\ operations$
      \EndIf
      \For{$l=N\ to\ 1$}
         \If{$l=N$}
         \State $\alpha_{k}^N=\frac{\partial S}{\partial x_{k}^{N}}$
         \State $z_{k}^N=\delta(\alpha_{k}^N)$
         \State update \ $\alpha_{k}'^N=z_{k}^N*\alpha_{k}^N$
         \State update \ $S\rightarrow S_{N}=\begin{matrix}\sum_{k}^N\alpha_{k}'^Nx_{k}^N \end{matrix}$
         \Else
         \State Fix\ $z_{k}^N,z_{k}^{N-1},\ldots,z_{k}^{l+1}$
         \State $S_{l+1}=\begin{matrix}\sum_{k}^{l+1}\alpha_{k}'^{l+1}x_{k}^{l+1} \end{matrix}$
         \State $\alpha_{k}^l=\frac{\partial S_{l+1}}{\partial x_{k}^{l}}$
         \State $z_{k}^l=\delta(\alpha_{k}^l)$
         \State Update\ $\alpha_{k}'^l=z_{k}^l*\alpha_{k}^l$
         \State update \ $S_{l+1}\rightarrow S_{l}=\begin{matrix}\sum_{k}^l\alpha_{k}'^lx_{k}^l \end{matrix}$
         \EndIf
      \State $l--$
      \EndFor
    \EndFor

  \end{algorithmic}
\end{algorithm}

To prove that the FR algorithm (Algorithm 1) can obtain local optimum, we need to prove that $S$ will keep increasing after each iteration, which means that we should prove $S_N \leq S_1$.
Accordingly, we first prove that ${S_N \leq S_{N-1}}$, then demonstrate that ${S_l \leq S_{l-1}}$ under the assumption of ${S_{l+1} \leq S_{l}}$, which following the rules of mathematical induction method.\\
{\bf 1. When $l=N$: }\\
   Note that $S$ can be described by any ReLU layer neuron.
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S=F(I_{0},Z)=\begin{matrix}\sum_{k}^N \alpha_{k}^N z_{k}^N x_{k}^N \end{matrix}}
      \end{array}
    \end{equation}
   $x_{k}^N$  is an output of the ReLU neuron in layer $N$, so $x_{k}^N \geq 0$.\\
   Note that
   \begin{equation}
      \begin{array}{*{20}{c}}
         {\alpha_{k}^N z_{k}^N x_{k}^N \leq \alpha_{k}^N \delta(\alpha_{k}^N) x_{k}^N}
      \end{array}
    \end{equation}
   Let $z_{k}^N \rightarrow z_{k}'^N =\delta(\alpha_{k}^N)$\\
   and
   \begin{equation}
      \begin{array}{*{20}{c}}
         {\alpha_{k}'^N=\alpha_{k}^N*z_{k}'^N, \ where\ \alpha_{k}'^N \geq 0}
      \end{array}
    \end{equation}
    then
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S \leq S_{N}=\begin{matrix}\sum_{k}^N \alpha_{k}'^N x_{k}^N \end{matrix}}
      \end{array}
    \end{equation}
    After updating all $z_{k}^N$ in layer $N$, $S_{N}$ can be expressed by layer $N-1$. Note that $\alpha_{k}^{N-1}$ is dependent on $\alpha_{k}^{N}$, and it will be changed to $\hat{\alpha}_{k}^{N-1}$ when $\alpha_{k}^N$ being modified, then
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S_{N}=\begin{matrix}\sum_{k}^{N-1} \hat{\alpha}_{k}^{N-1}z_{k}^{N-1}x_{k}^{N-1} \end{matrix}}
      \end{array}
    \end{equation}
   Update $z_{k}^{N-1}$ and $\hat{\alpha}_{k}^{N-1}$ to get $S_{N-1}$ in the same way when we update $z_{k}^{N}$ and $\alpha_{k}^{N}$, then
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S_N \leq S_{N-1}}
      \end{array}
    \end{equation}
 {\bf 2. Let us assume that $S_{l+1} \leq S_l$: }\\
    Fix\ $z_{k}^N, z_{k}^{N-1}, \ldots, z_{k}^{l+1}$, then
    \begin{equation}
      \begin{aligned}
         S_{l}=\begin{matrix}\sum_{k}^l \alpha_{k}'^l x_{k}^l \end{matrix}
      \end{aligned}
    \end{equation}
    Note that $x_{k}^l$ can be expressed by $x_{k'}^{l-1}$ with convolutional weights $w_{k'}^{l-1}$:
    \begin{equation}
      \begin{aligned}
         x_{k}^l=relu(\begin{matrix}\sum_{k'}^{l-1} w_{k'}^{l-1} z_{k'}^{l-1} x_{k'}^{l-1} \end{matrix})
      \end{aligned}
    \end{equation}
    If $\begin{matrix}\sum_{k'}^{l-1} w_{k'}^{l-1} z_{k'}^{l-1} x_{k'}^{l-1} \end{matrix} < 0 $, there will be a zero term in $S_l$ which can be ignored. So we just care about the case when
    $\begin{matrix}\sum_{k'}^{l-1} w_{k'}^{l-1} z_{k'}^{l-1} x_{k'}^{l-1} \end{matrix} \geq 0$, then
    \begin{equation}
      \begin{array}{*{20}{c}}
         {x_{k}^l=\begin{matrix}\sum_{k'}^{l-1} w_{k'}^{l-1} z_{k'}^{l-1} x_{k'}^{l-1} \end{matrix}}
      \end{array}
    \end{equation}
    So,
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S_{l}=\begin{matrix}\sum_{k}^l \alpha_{k}'^l \begin{matrix}\sum_{k'}^{l-1} w_{k'}^{l-1} z_{k'}^{l-1} x_{k'}^{l-1} \end{matrix} \end{matrix}}
      \end{array}
    \end{equation}
    Note that $\alpha_{k}'^l \geq 0$, and then
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S_{l}=\begin{matrix}\sum_{k'}^{l-1}(\begin{matrix}\sum_{k}^l \alpha_{k}'^l w_{k'}^{l-1}\end{matrix}) z_{k'}^{l-1} x_{k'}^{l-1} \end{matrix}}
      \end{array}
    \end{equation}
    Next, update the gates of ReLU layer $l-1$ based on $S_{l}$.\\
    Note that
    \begin{equation}
      \begin{array}{*{20}{c}}
         {\alpha_{k'}^{l-1}=\frac{\partial S_{l}}{\partial x_{k'}^{l-1}}=(\begin{matrix}\sum_{k}^l \alpha_{k}'^l w_{k'}^{l-1}\end{matrix})z_{k'}^{l-1}}
      \end{array}
    \end{equation}
    Update $z_{k'}^{l-1}\rightarrow z_{k'}'^{l-1}$
    \begin{equation}
      \begin{array}{*{20}{c}}
         z_{k'}'^{l-1}=\delta(\frac{\partial S_{l}}{\partial x_{k'}^{l-1}})
      \end{array}
    \end{equation}
    and $\alpha_{k'}^{l-1}\rightarrow \alpha_{k'}'^{l-1}$
    \begin{equation}
      \begin{aligned}
         \alpha_{k'}'^{l-1}&=(\begin{matrix}\sum_{k}^l \alpha_{k}'^l w_{k'}^{l-1}\end{matrix})*\delta(\begin{matrix}\sum_{k}^l \alpha_{k}'^l w_{k'}^{l-1}\end{matrix})\\
         &=\frac{\partial S_{l}}{\partial x_{k'}^{l-1}}*\delta(\frac{\partial S_{l}}{\partial x_{k'}^{l-1}})\\
         &=(\begin{matrix}\sum_{k}^l \alpha_{k}'^l w_{k'}^{l-1}\end{matrix})*z_{k'}'^{l-1}\\
         &\geq (\begin{matrix}\sum_{k}^l \alpha_{k}'^l w_{k'}^{l-1}\end{matrix})*z_{k'}^{l-1}
      \end{aligned}
    \end{equation}
    note that $\alpha_{k'}'^{l-1} \geq 0$ and $x_{k'}^{l-1} \geq 0$, so
   \begin{equation}
      \begin{aligned}
         S_{l-1} & =\begin{matrix}\sum_{k'}^{l-1} \alpha_{k'}'^{l-1} x_{k'}^{l-1} \end{matrix}\\
                 & \geq \begin{matrix}\sum_{k'}^{l-1}(\begin{matrix}\sum_{k}^l \alpha_{k}'^l w_{k'}^{l-1}\end{matrix}) z_{k'}^{l-1} x_{k'}^{l-1} \end{matrix}=S_l
      \end{aligned}
    \end{equation}
   That is
    \begin{equation}
      \begin{aligned}
         S_l \leq S_{l-1}
      \end{aligned}
    \end{equation}
   So based on the above mathematical induction, after the first iteration, the following conclusion can be drawn:
    \begin{equation}
      \begin{aligned}
         S_N \leq S_1
      \end{aligned}
    \end{equation}
   The score $S$ will keep increasing until convergence.
We take Fig.~\ref{fig:frfsp} as an example for explanation.
As shown in the first row of Fig.~\ref{fig:frfsp}, we embed the proposed FR algorithm into the VggNet~\cite{vgg} pre-trained on the ImageNet 2012 data set.
Given the input image in Fig.~\ref{fig:frfsp}(a) which contains an elephant and a zebra, we run FR for these two categories, respectively.
After convergence, a back-propagation to the image space is performed to acquire the visualization maps.
The results are depicted in Fig.~\ref{fig:frfsp}(b) and (c).
We find that the FR fails to distinguish particular patterns for different target objects, but can roughly restore the visual information lying on the receptive field of a target neuron, which is the reason that Algorithm 1 is named as the Feedback Recovering Algorithm.
A major reason for these results is that we sequentially change SWs of hidden neurons throughout the optimization process. Note that SW indicates how much a hidden neuron contribute to the target neuron.
More detailed analysis will be provided in the discussion section.
\begin{figure}[!tp]
\subfigure[input image]{
\label{fig:fr:a} %% label for first subfigure
\includegraphics[width=1.05in,height=1in]{figure/model/fr_fsp_ori.pdf}}
\hspace{0.005in}
\subfigure[FR for elephant]{
\label{fig:fr:b} %% label for second subfigure
\includegraphics[width=1.05in,height=1in]{figure/model/fr_elephant.pdf}}
\hspace{0.005in}
\subfigure[FR for zebra]{
\label{fig:fr:c} %% label for second subfigure
\includegraphics[width=1.05in,height=1in]{figure/model/fr_zebra.pdf}}

\subfigure[input image]{
\label{fig:fsp:d} %% label for first subfigure
\includegraphics[width=1.05in,height=1in]{figure/model/fr_fsp_ori.pdf}}
\hspace{0.005in}
\subfigure[FSP for elephant]{
\label{fig:fsp:e} %% label for second subfigure
\includegraphics[width=1.05in,height=1in]{figure/model/fsp_elephant.pdf}}
\hspace{0.005in}
\subfigure[FSP for zebra]{
\label{fig:fsp:f} %% label for second subfigure
\includegraphics[width=1.05in,height=1in]{figure/model/fsp_zebra.pdf}}
\caption{Results by running the FR and FSP for different targets respectively. (a) The input image for FR. (b)(c) Output maps via running FR for elephant and zebra respectively. (d) The same input image for FSP. (e)(f) Output maps via running FSP for elephant and zebra respectively. Best viewed in color.}
\label{fig:frfsp} %% label for entire figure
\end{figure}
\subsubsection{Feedback Selective Pruning}
The fact that FR has weak discriminative ability by modulating SWs during its optimization reminds us to determine the additional gates $Z$ by unchanged SWs.
With this assumption, we modulate the input of each hidden neuron to maximize the target $S$.
In particular, we optimize all the gates $Z$ layer-by-layer in a bottom-up order as illustrated in Algorithm 2.
\begin{algorithm}
  \renewcommand\baselinestretch{1.5}\selectfont
  \small
  \small
  \caption{The Feedback Selective Pruning Algorithm}
 \begin{algorithmic}[]
   \State $\textbf{INPUT}:\ image\ I_{0}$, $target \ neuron\ with\ score\ function\ S$
   \State $\textbf{DO}:$
   \State $Initialize\ all\ Z\ with\ 1$
    \For {$iteration=1\ to\ max\ iteration$}
      \State $feedforward$
      \If{$iteration==1$}
      \State $Do \ linear\ approximation\ operations$
      \EndIf
      \For{$l=1\ to\ N$}
         \If{$l=1$}
         \State $\alpha_{k}^1=\frac{\partial S}{\partial x_{k}^{1}}$
         \State $z_{k}^1=\delta(\alpha_{k}^1)$
         \State update \ $x_{k}'^1=z_{k}^1*x_{k}^1$
         \State update \ $S\rightarrow S_{1}=\begin{matrix}\sum_{k}^1\alpha_{k}^1x_{k}'^1 \end{matrix}$
         \Else
         \State Fix\ $z_{k}^l,z_{k}^{2},\ldots,z_{k}^{l-1}$
         \State $S_{l-1}=\begin{matrix}\sum_{k'}^{l-1}\alpha_{k'}^{l-1}x_{k'}'^{l-1} \end{matrix}$
         \State  and also,
         \State $S_{l-1}=\begin{matrix}\sum_{k}^{l}\alpha_{k}^{l}x_{k}^{l} \end{matrix}$
         \State $x_{k}^l=relu(\begin{matrix}\sum_{k'}^{l-1} w_{k'}^{l-1} z_{k'}^{l-1} x_{k'}'^{l-1} \end{matrix})$
         \State $\alpha_{k}^l=\frac{\partial S_{l-1}}{\partial x_{k}^{l}}$
         \State $z_{k}^l=\delta(\alpha_{k}^l)$
         \State Update\ $x_{k}'^l=z_{k}^l*x_{k}^l$
         \State update \ $S_{l-1}\rightarrow S_{l}=\begin{matrix}\sum_{k}^l\alpha_{k}^lx_{k}'^l \end{matrix}$
         \EndIf
      \State $l++$
      \EndFor
    \EndFor

  \end{algorithmic}
\end{algorithm}
To prove that the FSP algorithm (Algorithm 2) can also reach a local optimum, the mathematical induction method is adopted again.
In this case, we need to prove $S_1 \leq S_N$.
We first prove that ${S_1 \leq S_{2}}$, and then illustrate ${S_l \leq S_{l+1}}$ under the assumption of ${S_{l-1} \leq S_{l}}$.\\ \\
 {\bf 1. When $l=1$: }\\
    Note that
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S=F(I_{0},Z)=\begin{matrix}\sum_{k}^1 \alpha_{k}^1 z_{k}^1 x_{k}^1 \end{matrix}}
      \end{array}
    \end{equation}
    \\
   $x_{k}^1$  is an output of a ReLU neuron in layer 1, so $x_{k}^1 \geq 0$.\\
   Note that
    \begin{equation}
      \begin{array}{*{20}{c}}
         {\alpha_{k}^1 z_{k}^1 x_{k}^1 \leq \alpha_{k}^1 \delta(\alpha_{k}^1) x_{k}^1}
      \end{array}
    \end{equation}
   Let $z_{k}^1 \rightarrow z_{k}'^1 =\delta(\alpha_{k}^1)$, then
    \begin{equation}
      \begin{array}{*{20}{c}}
         {x_{k}'^1=x_{k}^1*\delta(\alpha_{k}^1), \ note\ that\ x_{k}'^1 \geq 0}
      \end{array}
    \end{equation}
    and thus
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S \leq S_{1}=\begin{matrix}\sum_{k}^1 \alpha_{k}^1 x_{k}'^1 \end{matrix}}
      \end{array}
    \end{equation}
    After update all $z_{k}^1$ in layer 1, $S_{1}$ can be expressed by layer 2. Note that $x_{k}^{2}$ will be changed to $\hat{x}_{k}^{2}$ because of $x_{k}^1$ being modified, that is
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S_{1}=\begin{matrix}\sum_{k}^2 \alpha_{k}^2 z_{k}^2 \hat{x}_{k}^2 \end{matrix}}
      \end{array}
    \end{equation}
    Update $z_{k}^{2}$ and $\hat{x}_{k}^{2}$ to get $S_{2}$ with the same way when we update $z_{k}^{1}$ and $x_{k}^{1}$, therefore:
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S_1 \leq S_{2}}
      \end{array}
    \end{equation}
{\bf 2.	Let us assume that $S_{l-1} \leq S_l$: }\\
    Fix\ $z_{k}^1, z_{k}^{2}, \ldots, z_{k}^{l-1}$, and then
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S_{l}=\begin{matrix}\sum_{k}^l \alpha_{k}^l x_{k}'^l \end{matrix}}
      \end{array}
    \end{equation}
    The score $S$ can be expressed by $x_{k}^{l+1}$, so
    \begin{equation}
      \begin{array}{*{20}{c}}
         {S_{l}=\begin{matrix}\sum_{k}^{l+1} \alpha_{k}^{l+1} \hat{x}_{k}^{l+1} z_{k}^{l+1} \end{matrix}}
      \end{array}
    \end{equation}
    where
    \begin{equation}
      \begin{array}{*{20}{c}}
         {\hat{x}_{k}^{l+1}=relu(\begin{matrix}\sum_{k'}^{l} w_{k'}^{l} z_{k'}^{l} x_{k'}'^{l} \end{matrix})}
      \end{array}
    \end{equation}
    and thus, \ $\hat{x}_{k}^{l+1} \geq 0$\\
    Because
    \begin{equation}
      \begin{aligned}
         S_l & =\begin{matrix}\sum_{k}^{l+1} \alpha_{k}^{l+1} \hat{x}_{k}^{l+1} z_{k}^{l+1}\end{matrix}\\
          & \leq \begin{matrix}\sum_{k}^{l+1} \alpha_{k}^{l+1} \delta(\alpha_{k}^{l+1})\hat{x}_{k}^{l+1} \end{matrix}\\
          &=\begin{matrix}\sum_{k}^{l+1} \alpha_{k}^{l+1} x_{k}'^{l+1} \end{matrix}=S_{l+1}
      \end{aligned}
    \end{equation}
    that is, $S_l\leq S_{l+1}$\\
    Update $\hat{x}_{k}^{l+1} \rightarrow x_{k}'^{l+1}$ and $z_{k}^{l+1} \rightarrow z_{k}'^{l+1}$ with
    \begin{equation}
      \begin{aligned}
        x_{k}'^{l+1}&=\hat{x}_{k}^{l+1}\delta(\alpha_{k}^{l+1})\\
        z_{k}'^{l+1}&=\delta(\alpha_{k}^{l+1})\\
    \end{aligned}
    \end{equation}
   Based on the above mathematical induction methods, after the first iteration, we have
   \begin{equation}
      \begin{aligned}
         S_1 \leq S_N
      \end{aligned}
    \end{equation}
The target $S$ will keep increasing until convergence.

As shown in Fig.~\ref{fig:frfsp}, we apply the FSP for elephant and zebra using the same input image and VggNet, respectively.

Fig.~\ref{fig:frfsp}(e) and (f) illustrate the results.
It is obvious that compared with the FR algorithm, the FSP algorithm has much more powerful discriminative ability between different target objects.
The salient regions in Fig.~\ref{fig:fsp:e} and Fig.~\ref{fig:fsp:f} are focused on different targets separately, although the visualization maps contains incomplete objects.
Hence, the FSP algorithm has the capacity to select neurons in deep CNN according to a pre-defined label, and that is why we call this algorithm as Feedback Selective Pruning.
The main reason for these results is that the states of gate variables are determined by the SWs of hidden neurons and the inputs are modified instead of the SWs during the optimization process.
We will further discuss this in the next section.
\subsection{Discussion}
As mentioned before, hidden neurons in a deep CNN are represented for particular patterns in the image space~\cite{zeiler,deepinside,zhou2014object}.
Also we show, in Equation~(3), that a hidden neuron denoted by $x_{ijc}^l$ can represent a particular pattern which is expressed by the weights of pathways between the related input pixels and this neuron.
In particular, the strength of $x_{ijc}^l$ is the similarity measurement when given an input pattern, and SW, namely $\alpha_{ijc}^l$, reflects how this input pattern contributes to the target neuron $S$.

To maximize the target score $S$, the FR algorithm updates SWs layer-by-layer from the top to the bottom.
During optimization, the additional gate $z_{ijc}^l$ for $x_{ijc}^l$ is determined by the modified $\hat{\alpha}_{ijc}^l$ via $\delta(\hat{\alpha}_{ijc}^l)$ instead of original $\alpha_{ijc}^l$.
As a consequence, the updating of $\alpha_{ijc}^l$ destroys the discriminative ability of the target neuron to judge whether a pattern is beneficial to semantic information it represents.
But interestingly, the FR algorithm provides a good method to visualize the content in the receptive field of a neuron, which can be a very good supplement to the FSP for generating practical energy maps.

In contrast, the FSP algorithm updates the activations of hidden neurons to maximize the target score $S$ from the bottom to the top layer.
For a particular neuron $x_{ijc}^l$, its gate status is determined by $\delta(\alpha_{ijc}^l)$, although the value of $x_{ijc}^l$ may have changed during updating $x_{ijc}^{l-1}$.
From another point of view, $S$ can be reinterpreted in the following way:
\begin{equation}
\begin{aligned}
  S=\frac{1}{N}\sum_{l=1}^N\begin{matrix}\sum_{ijc}^l \alpha_{ijc}^l x_{ijc}^l \end{matrix}
\end{aligned}
\end{equation}
If all $x_{ijc}^l \ \forall \ c, l\in {1,2,\ldots,N}$ represent different patterns, then $S$ is a linear combination of all those patterns, as demonstrated in Equation (35).
All the patterns with the negative SW will be erased by FSP.
This will change the values of the preserved $x_{ijc}^l$, but it will not change the relationship between the preserved patterns and the target neuron.
\begin{figure}[!tp]
\vspace{0mm}
\centering
\includegraphics[width = 1 \linewidth,height =1 \linewidth]{figure/model/vis.pdf}
\caption{Results generated by combining both the FSP and FR. (a) Input images. (b)(c) Merged energy maps and visualization maps by running FR separately on neurons selected by FSP. (d)(e) Energy and visualization maps by running FR simultaneously on neurons selected by FSP. Best viewed in color.}
\label{fig:vis}
\end{figure}
The FSP naturally offers a good way to seek patterns closely related to particular target objects.

Because neither FR nor FSP can provide a global optimum solution, it is difficult to obtain perfect visualization and energy maps only by either of them.
Fortunately, combined with the advantages of both, the proposed Feedback CNN can produce impressive results.
In Feedback CNN, the FSP algorithm is utilized to select target-relevant neurons, and the FR algorithm is employed to reconstruct the target object in the image space.
Fig.~\ref{fig:vis} presents some specific examples.
Specially, Fig.~\ref{fig:vis}(b) and (c) are generated by running FR on selected neurons separately and merging the energy and the visualization maps together.
Fig.~\ref{fig:vis}(d) and (e) are obtained by running FR on selected neurons simultaneously which is much more efficient.
As can be seen, objects are captured by Feedback CNN even when they are hidden in cluttered backgrounds.
And neurons related with the target objects will be selected but irrelevant neurons will be turned off.
Note that this kind of selectivity occurs in every hidden layer.
%It is worth noting that a deep CNN is difficult to be trained perfectly, and noisy patterns may have positive SW.
%Therefore, it is practical to set a threshold to determine the states of gate variables by the mean value of all SWs in a layer.

\section{Experiments}
In this section, extensive experiments are carefully designed to verify the effectiveness of the proposed Feedback CNN.
The iteration process of FSP is analyzed in Section 4.1 and the effectiveness of neuron selection is studied in Section 4.2.
We evaluate the discriminative ability of FSP in Section 4.3.
Besides, we conduct quantitative experiments of weakly supervised object localization in Section 4.4 and weakly supervised semantic segmentation in Section 4.5.
It should be noted that since the FR algorithm is like a kind of image reconstruction, we evaluate FR together with FSP in Section 4.4 and Section 4.5.

\subsection{Analysis on Iteration Process of FSP}
\begin{figure}
\subfigure[dog and cat]{
\label{fig:iteration:a} %% label for first subfigure
\includegraphics[width=1in,height=1in]{figure/experiments/iteration/dogcat.pdf}}
\hspace{0.005in}
\subfigure[iteration for dog]{
\label{fig:iteration:b} %% label for second subfigure
\includegraphics[width=1in,height=1in]{figure/experiments/iteration/dog.pdf}}
\hspace{0.005in}
\subfigure[iteration for cat]{
\label{fig:iteration:c} %% label for second subfigure
\includegraphics[width=1in,height=1in]{figure/experiments/iteration/cat.pdf}}

\subfigure[bus and cars]{
\label{fig:iteration:d} %% label for second subfigure
\includegraphics[width=1in,height=1in]{figure/experiments/iteration/buscar.pdf}}
\hspace{0.005in}
\subfigure[iteration for bus]{
\label{fig:iteration:e} %% label for second subfigure
\includegraphics[width=1in,height=1in]{figure/experiments/iteration/bus.pdf}}
\hspace{0.005in}
\subfigure[iteration for cars]{
\label{fig:iteration:f} %% label for second subfigure
\includegraphics[width=1in,height=1in]{figure/experiments/iteration/car.pdf}}
\caption{The iteration curve of FSP for different objects. (a)(d) Input images containing multiple objects. (b)(c) The iteration curves for dog and cat respectively with the input of (a). (e)(f) The iteration curve for bus and car respectively with the input of (d). Best viewed in color.}
\label{fig:iteration} %% label for entire figure
\end{figure}

In order to verify our theoretical analysis described in Section 3 that the score of the target neuron would keep increasing until convergence when running the FSP algorithm, we specially visualize the iterative process of the FSP algorithm here.
For experimental purposes, the VggNet (16 layers)~\cite{vgg}, which is obtained from Caffe~\cite{caffe} Model Zoo website and pre-trained with ImageNet 2012 training set, is fine-tuned on the Pascal VOC2012 data set.

Firstly, as shown in Fig.~\ref{fig:iteration}(a), (b) and (c), given the input image, the FSP algorithm is applied respectively on two neurons which represent the categories of ``dog" and ``cat" in the last fully connected layer named as ``fc8" in the VggNet.
The scores of all the 20 neurons in ``fc8", corresponding to the 20 classes of Pascal VOC2012, are recorded during the iteration procedure.
The iteration curve for category ``cat" is plotted with the red line, ``dog" with the green line, and other 18 classes with the blue lines.
As can be seen, all the iterative procedure converge after about 5 iterations.
And the scores of the target neuron keep increasing until convergence, while the scores of other classes are suppressed even if the corresponding objects are presented in the image.
The same results happen when given an image contains a bus and several cars, as shown in Fig.~\ref{fig:iteration}(d), (e) and (f).
These results prove that FSP will converge to a local optimum efficiently, and effectively increase the score of the target neuron.

Furthermore, the FSP algorithm is applied on the ImageNet 2012 classification validation set which contains 50000 images.
The ground-truth label of each image is set as the target for the feedback model, and the scores of 5 iterations for all images are recorded.
Then we calculate their mean and standard deviation score of each iteration, and plot them in Fig.~\ref{fig:mean}.
We find that the FSP algorithm is also effective even for a very large image data set.

In addition, it should be noted that there are several small cars in the top left and right corners in Fig.~\ref{fig:iteration}(d).
When feedback is applied with respect to category ``car", the scores of the target neuron keep increasing while the scores for  ``bus'' decrease heavily although there is a big bus in the center of the image, as demonstrated in Fig.~\ref{fig:iteration}(f).
The reason is that neurons carrying useful information for particular targets can be selected effectively while irrelevant neurons will be turned off in the feedback loops.
\begin{figure}[!tp]
\vspace{0mm}
\centering
\includegraphics[width = 6cm]{figure/experiments/iteration/mean.pdf}
\caption{The mean iteration curve of 50000 images from the ImageNet 2012 classification validation set.}
\label{fig:mean}
\end{figure}
More analysis will be presented in the next experiment.
\begin{figure*}[!tp]
\setlength{\abovecaptionskip}{0pt}
\subfigure[input image]{
\label{fig:effect:a} %% label for first subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/three.pdf}}
\subfigure[person(max)]{
\label{fig:effect:b} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/maxperson.pdf}}
\subfigure[car(max)]{
\label{fig:effect:c} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/maxcar.pdf}}
\subfigure[bicycle(max)]{
\label{fig:effect:d} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/maxbicycle.pdf}}
\subfigure[input image]{
\label{fig:effect:e} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/two.pdf}}
\subfigure[person(max)]{
\label{fig:effect:f} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/maxperson2.pdf}}
\subfigure[bottle(max)]{
\label{fig:effect:g} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/maxbottle.pdf}}
\centering

\subfigure[input image]{
\label{fig:effect:h} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/three.pdf}}
\subfigure[person(mean)]{
\label{fig:effect:i} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/sumperson.pdf}}
\subfigure[car(mean)]{
\label{fig:effect:j} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/sumcar.pdf}}
\subfigure[bicycle(mean)]{
\label{fig:effect:k} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/sumbicycle.pdf}}
\subfigure[input image]{
\label{fig:effect:l} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/two.pdf}}
\subfigure[person(mean)]{
\label{fig:effect:m} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/sumperson2.pdf}}
\subfigure[bottle(mean)]{
\label{fig:effect:n} %% label for second subfigure
\includegraphics[width=0.95in,height=0.85in]{figure/experiments/effectiveness/sumbottle.pdf}}
\centering
\caption{Filter selection. Given the input images like (a) and (e), the FSP algorithm is run for different objects in these images, and we select top 5 channels for each target object according to the maximum scores of 512 channels in the ``conv5\_2" layer. Those 5 channels correspond to 5 filters. We feed the images of 20 different classes of Pascal VOC2012 validation set to CNN, and calculate the maximum and mean scores of those 5 filters corresponding to the images of different classes. The first row reports the maximum scores curve and the second row reports the mean scores curve. The filters selected by FSP  well respond to the corresponding class images. For example, the selected filters for ``person" have much higher responses to the images from the category ``person". Best viewed in color. }
\label{fig:effect} %% label for entire figure
\end{figure*}


\subsection{Effectiveness of Neuron Selection}
In cognitive science, visual attention in the Biased Competition Theory~\cite{beck2009top,desimone1998visual,desimone1995neural} is explained as that human visual cortex is enhanced by top-down stimuli, and irrelevant neurons will be suppressed in feedback loops when searching for objects.
When applying the FSP algorithm to the CNN model, we find that the results are similar to that described above.

Given an image with multiple classes, we run the FSP algorithm with the same VggNet in Section 4.1 for different targets, and focus on a middle layer named ``conv5\_2" which lies in the 12th layer of the total 16 layers.
This layer has 512 filter kernels, indicating that it may express 512 patterns related to different classes.
\begin{figure}[!tp]
\vspace{0mm}
\centering
\includegraphics[width = 1.0 \linewidth, height = 0.7 \linewidth]{figure/experiments/effectiveness/top5.png}
\caption{Visualization of 5 neurons that have the maximum scores in each of the top 5 channels selected by the FSP algorithm. Those 5 neurons express the most discriminative parts of the corresponding target objects in the input image. Best viewed in color.}
\label{fig:top 5}
\end{figure}

As shown in Fig.~\ref{fig:top 5}, the first input image contains persons, a bicycle and a car, and the FSP algorithm is run for the three classes.
After convergence, for each target, we rank the 512 channels of the ``conv5\_2" layer with their maximum scores and acquire the top 5 channels.
Further, 5 neurons are selected that have the maximum activation scores in each of the top 5 channels.
As illustrated in Fig.~\ref{fig:top 5}, the FR algorithm is run to visualize those 5 neurons selected by FSP, and it turns out that they represent the most discriminative parts of the corresponding objects.
The similar results appear in another image with category person and bottle.

To make it more convincing, we evaluate these selected top 5 filters on the whole Pascal VOC2012 classification validation set which contains 1449 images.
To avoid the underlying mutual influence, only 924 images with a single label are utilized, and these images are divided into 20 sets according to their labels.
We calculate the maximum and mean responses of the top 5 filters with all images of 20 categories.
In Fig.~\ref{fig:effect}, the maximum and mean responses are presented in the first row and the second row respectively.
We take the category ``person" as an example for detailed analysis.
FSP is run for the category ``person" on the person-bike-car image until convergence, and top 5 filters are acquired.
All images that are only labeled as person are fed to the original CNN model.
The responses are drawn with the magenta line.
Then, images of the other 19 classes are fed to the same CNN to get the corresponding responses of the top 5 filters.
The responses for the category ``bicycle" and ``car" that appear in the image are plotted with the red and cyan line respectively, and the rest 17 classes are plotted with blue lines.

In Fig.~\ref{fig:effect:b}, the fact that the magenta line is higher than the other lines indicates that the corresponding filters are highly related to its target category, which means the FSP algorithm can effectively select the meaningful filters.
The results are similar for another image that contains persons and  bottles, as shown in Fig.~\ref{fig:effect:e}.
We find that this kind of neuron selection happens in all hidden layers, based on which we can draw a conclusion that the FSP algorithm has the ability to correctly select the corresponding neurons (filters) to preset targets, as well as suppress irrelevant neurons at the same time.
\begin{figure*}[!tp]
\vspace{0mm}
\centering
\includegraphics[width = 1.0 \linewidth,height=0.4 \linewidth]{figure/experiments/distinguish/all.pdf}
\caption{Visualization and energy maps. (a)(e)(i)(m) Input images. (b)(f)(j)(n) FSP-FR energy maps. (c)(g)(k)(o) Summation Energy Maps. (d)(h)(l)(p) Visualization maps. Note that (i-p) demonstrate some results when input images contain multi-class objects, and the Summation Energy Maps are generated by summation of all resized gradients of feature maps in all ReLU layers that behind convolutional layers. Best viewed in color.}
\label{fig:energymap}
\end{figure*}

\subsection{Analysis on the Discriminative Ability of FSP}
The FSP algorithm can produce highly discriminative energy maps, which is essential for real applications.
To evaluate this discriminative ability of FSP, we conduct several experiments on the Pascal VOC2012 classification validation set.
The same VggNet in Sec 4.1 is employed, and Feedback CNN is utilized to generate category-specific energy maps.

As a result, Fig.~\ref{fig:energymap} depicts several examples.
The energy maps generated by Feedback CNN are highly relevant to the target objects in the input images, as shown in Fig.~\ref{fig:energymap}(b), (f), (j) and (n).
For convenience, these energy maps are named as FSP-FR energy maps.

Due to the selection ability of the FSP algorithm, most of the neurons preserved in all hidden layers are highly relevant to the same semantic class.
Meanwhile, a whole object can be divided into several parts which may be expressed in several different hidden layers.
Thus, it is reasonable to combine all the selected neurons to generate an new energy map.
We achieve this simply by summation operations.
After applying the FSP algorithm, we resize the gradients of feature maps in all ReLU layers that behind convolutional layers with the same size of the input image, and calculate the summation of all the resized gradient maps along the channel direction.
The summation map is normalized by L2 norm, named as Summation Energy Map.
Fig.~\ref{fig:energymap}(c), (g), (k) and (o) illustrate some results.
As can be seen, the Summation Energy Maps have a better distributions over target objects.

\begin{figure}[!htb]
\vspace{0mm}
\centering
\includegraphics[height = 3.5cm,width = 1\linewidth]{figure/experiments/coverrate/all_diff.pdf}
\caption{Coverage rates of Summation Energy Map over all 20 classes images of Pascal VOC2012. (left) Coverage rates of Summation Energy Maps. (right) Coverage rates of the energy maps generated by original gradients. Best viewed in color.}
\label{fig:alldiff}
\end{figure}

\begin{figure}[!htb]
\vspace{0mm}
\centering
\includegraphics[height = 3.5cm,width = 1\linewidth]{figure/experiments/coverrate/mul_diff.pdf}
\caption{Coverage rates of Summation Energy Map of multi-class images of Pascal VOC2012. (left) Coverage rates of Summation Energy Maps. (right) Coverage rates of the energy maps generated by original gradients. Best viewed in color.}
\label{fig:muldiff}
\end{figure}

The Summation Energy Map integrates the information of the selected neurons in all hidden layers.
So it is more convincing that we evaluate the discriminative ability of FSP using Summation Energy Maps instead of FSP-FR energy maps.
We calculate the Summation Energy Maps for each image of the Pascal VOC2012 segmentation validation set.
As the data set provides ground-truth masks for each object in every image according to class labels, we calculate the sum of energy that falls into the target object regions, and we call this value as the coverage rate.
The mean coverage rate is computed for each class on all provided validation images in Fig.~\ref{fig:alldiff} with blue curve.
Specially, since the deconvolutional operation in back-propagation cause dilation of the edges in the energy maps, we further report the results of dilating the ground truth masks by 5 pixels and 10 pixels in Fig.~\ref{fig:alldiff} with green and red curves respectively.
By contrast, the mean coverage rate of energy maps which depends on the original gradients of the input image is reported too.
As can be seen, all coverage rates of Summation Energy Maps (left) is much higher than the energy maps generated by original gradients of the input images (right).
That is, the Summation Energy Maps generated by FSP effectively highlight the expected objects and almost focus on the target area.

Particularly, to provide a more convincing evaluation of the discriminative ability of Summation Energy Map, we calculate the coverage rate only for images with multi-class labels in PASCAL VOC2012 segmentation validation set.
The corresponding results are shown in Fig.~\ref{fig:muldiff}, demonstrating the effectiveness of FSP.

These results indicate that the propose FSP algorithm has strong discriminative ability.
Object-related neurons can be correctly selected by FSP and class-specific energy maps can also be effectively produced, which well paves the road for weakly supervised object localization and weakly-supervised semantic segmentation.

\subsection{Weakly-supervised Object Localization}
In this section, we evaluate the localization power of Feedback CNN on the ImageNet 2012 localization task.
The top 5 localization evaluation metric~\cite{deepinside} is employed, in which an correct prediction is counted when one of the top 5 guesses meets the requirement that both object category prediction and its associated bounding box are correct.
To generate top 5 category prediction, the VggNet pre-trained with the ImageNet 2012 classification training set is downloaded from Caffe~\cite{caffe} Model Zoo website.
For weakly supervised localization, several steps are performed to get a bounding box.
We summarize the experimental procedures of weakly supervised object localization in Procedure 1.

\begin{procedure}[!htb]
\caption{Experimental procedures of weakly supervised object localization}
\label{alg:Framwork}
\begin{algorithmic}[1]
%\renewcommand{\algorithmcfname}{Experimental procedures}
\State Given an image and a predicted label;
\State Subtract the mean values and resize the image to the size of 224*224 as the input;
\State Run FSP according to the predicted label and obtain Summation Energy Map;
\State Get a bounding box which preserves 99\% energy of the Summation Energy Map. Crop the box region from the original image and resize to 224*224 as the input;
\State Apply FSP again on the new input image;
\State Set one of the middle-level layers (e.g., ``conv5\_2") as the target layer for FR;
\State Set the preserved neurons in ``conv5\_2" as the targets, and run FR on those neurons simultaneously to get the final energy maps;
\State Get a bounding box which preserves 99\% energy of the final energy map.
\end{algorithmic}
\end{procedure}

\begin{table}[!htb]
\small
\small
\small
\renewcommand{\arraystretch}{1}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\caption{Localization results on ILSVRC2012.}
\label{table:localization results}
\centering
\begin{tabular}{c| p{1.5cm}<{\centering} |p{1.5cm}<{\centering}}
\hline
%\multirow{2}{*}{methods} & \multirow{2}{*}{Top5 classification error(\%)} &\multirow{2}{*}{Top5 localization error(\%)}\\
%{methods\\} & {Top5 classification \\error} &{Top5 localization \\error}\\
\multirow{2}{*}{methods}&{classification}&{localization}\\
%\cline{2-3}
& {top 5 error }&{top 5 error}\\
%\tabincell{p{1cm}}{ } & {classification}&{localization}\\
%\hline
%\tabincell{p{1cm}}{ methods} & {top 5 error }&{top 5 error} \\
\hline
 {deepinside~\cite{deepinside}} & {-}&{44.6}  \\
 {VGGnet-GAP~\cite{zhou2015learning}} & {12.2}&{45.14}  \\
 {Backprop-on-VGGnet~\cite{zhou2015learning}} & {11.4}&{51.46}  \\
 {GoogLeNet-GAP~\cite{zhou2015learning}} & {13.2}&{43.00}  \\
 {GoogLeNet~\cite{zhou2015learning}} & {11.3}&{49.34}  \\
 \hline
 {Feedback CNN-no crop } & {\bf 15.68}&{\bf 42.82}  \\
 {Feedback CNN-5 crop } & {\bf 12.95}&{\bf 41.72}  \\
 {Feedback CNN-dense crop } & {\bf 9.22}&{\bf 40.32}  \\
 \hline
 {MWP~\cite{zhang2016top}}&{with GT}&{ 38.70}\\
 {Feedback CNN} & {with GT}&{\bf 36.50}  \\
\hline
\end{tabular}
\end{table}

In particular, as described in Procedure 1, objects are localized in two stages for the reason that the scale of objects may vary greatly between different input images.
We first roughly localize the objects in Step 1-4 by running FSP.
Then the precise localization of objects is obtained in Step 5-9 by combining FSP and FR.
%Note that, the ``conv5\_2" layer is empirically selected as the target layer of FR for the best results.

We compare the localization performance of Feedback CNN on the ILSVRC2012 validation set with several state-of-the-art methods in Table~\ref{table:localization results}.
Different image cropping strategies, such as no cropping, 5 cropping, and dense cropping~\cite{vgg}, are employed to produce different classification accuracy.
When both using VggNet, compared with the VGGnet-GAP in~\cite{zhou2015learning}, our method wins 5.01\% in terms of the accuracy of weakly supervised object localization.
To avoid the influence of different classification performance of the compared models, we compare the localization accuracy when classification error rates are close.
As illustrated in Table~\ref{table:localization results}, When our classification accuracy is 3.48\% (without cropping operation) and 0.75\% (with 5 cropping operations) lower than the compared approach VGGnet-GAP~\cite{zhou2015learning}, we still achieve 2.32\% and 3.42\% higher localization accuracy, respectively.
Moreover, when given ground truth labels, 36.50\% error rate is obtained, which is an accuracy of 2.2\% higher than the recent best-performing approach MWP~\cite{zhang2016top} under the same experimental set-up.

Due to the powerful selection capability of FSP and the better object boundaries in energy maps, the proposed Feedback CNN outperforms the state-of-the-art approaches (VGGnet-GAP~\cite{zhou2015learning} and MWP~\cite{zhang2016top}).
The energy maps generated by our Feedback CNN are more precise and contain more complete objects.
Accordingly, the bounding boxes generated by our approach are more closely to the ground-truth bounding boxes. Fig.~\ref{fig:loc} displays some successful examples.
\begin{figure}
\setlength{\abovecaptionskip}{0pt}
\includegraphics[width=1 \linewidth,height=0.4 \linewidth]{figure/experiments/localization/loc_results.pdf}
\centering
\caption{Examples of weakly supervised localization of our approach. Note that the predicted bounding boxes are plotted in red color.}
\label{fig:loc} %% label for entire figure
\end{figure}

\begin{table*}[!tp]
\small
\small
\small
\renewcommand{\arraystretch}{1.3}
\renewcommand{\tabcolsep}{0.04cm}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\arrayrulewidth=0.8pt %±í¸ñ±ß¿òµÄºñ¶È
\caption{Results of weakly supervised semantic segmentation on the Pascal Voc2012 validation dataset.}
\label{table:weakly segmentation}
\centering
\begin{tabular}{c|ccccccccccccccccccccc|c}
\hline
&bkg &plane &bike &bird & boat & bottle & bus & car & cat & chair & cow & table & dog & horse & motor & person & plant & sheep & sofa & train & tv & mAP\\
\hline
img+obj~\cite{bearman2015s} & & & & &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & & 32.2 \\
stage1~\cite{kim2016scale} &71.7 &30.7 &{\bf 30.5} & 26.3 & 20.0 & 24.2 & 39.2 & 33.7 & 50.2 & 17.1 & 29.7 & 22.5 & 41.3 & 35.7 & 43.0 & 36.0 & 29.0 & 34.9 & 23.1 & 33.2 & 33.2 & 33.6 \\
EM-Adapt~\cite{papandreou2015weakly} & 67.2 & 29.2 &  17.6 &  28.6 &  22.2 & 29.6 &  47.0 & 44.0 & 44.2 & 14.6 & 35.1 & {\bf 24.9} & 41.0 & 34.8 & 41.6 & 32.1 & 24.8 & 37.4 & 24.0 & 38.1 & 31.6 & 33.8 \\
CCNN~\cite{ccnn} & 68.5 & 25.5 & 18.0 & 25.4 & 20.2 & 36.3 & 46.8 & 47.1 & 48.0 & 15.8 & 37.9 & 21.0 & 44.5 & 34.5 & 46.2 & {\bf 40.7} & {\bf 30.4} & 36.3 & 22.2 & 38.8 & 36.9 & 35.3\\
MIL+ILP+SP~\cite{segcvpr15} & 77.2 & 37.3 & 18.4 & 25.4 & 28.2 & 31.9 & 41.6 & 48.1 & 50.7 & 12.7 & 45.7 & 14.6 & 50.9 & 44.1 & 39.2 & 37.9 & 28.3 & 44.0 & 19.6 & 37.6 & 35.0 & 36.6 \\
\hline
\bf ours &{\bf 81.1}&{\bf 62.1}&25.9 & {\bf 51.5} & {\bf 32.5} & {\bf 47.7} & {\bf 57.7} & {\bf 51.0} & {\bf 65.1} & {\bf 20.6} & {\bf 55.6} & 23.7 &	{\bf 54.5} & {\bf 54.6} & {\bf 57.3} & 38.5 & 27.2 & {\bf 65.9} & 31.2 & {\bf 50.7} & {\bf 40.3}&{\bf 47.4}\\
\hline
\end{tabular}
\end{table*}

\begin{figure*}[!htb]
\vspace{0mm}
\centering
%\includegraphics[width = 1\linewidth,height = 1.15\linewidth]{figure/experiments/segmentation/good.pdf}
\includegraphics[width = 1\linewidth]{figure/experiments/segmentation/good2.pdf}
\caption{Examples of weakly supervised semantic segmentation on the Pascal VOC2012 validation set. The first row are input images, the second row are ground truth, and the last row are our results.}
\label{fig:goodseg}
\end{figure*}
\subsection{Weakly-supervised Semantic Segmentation}
In this section, we focus on the weakly-supervised semantic segmentation task with experimental analysis on the Pascal VOC2012 semantic segmentation Challenge.
We employ the standard Pascal VOC2012 segmentation metric: mean intersection-over-union (mIoU).
Note that we only make use of class-level labels to fine-tune VggNet for classification on the Pascal VOC2012 segmentation training set, and evaluate our method on the Pascal VOC2012 semantic segmentation validation set (containing 1449 images).
In the training phase, the input images are randomly cropped, mirrored, scaled and rotated to obtain a better model.
As for the multi-label classification task, the loss function we adopt is the sigmoid cross entropy instead of soft-max.
To segment objects from an input image based on the energy map, the saliency cut proposed in ~\cite{saliencycut} is utilized.

Procedure 2 demonstrates the experimental procedure.
In particular, distinct parts of an object may be expressed in different layers, and their information can be all integrated into the Summation Energy Maps, which makes the Summation Energy Maps suitable for the segmentation task.
On the other hand, the FSP-FR energy maps can highlight object boundaries.
Thus, we acquire both these energy maps for the target objects in an input image and simply add them together as the final energy map, which is called Summation-FSP-FR energy map.
Specially, for the overlapped objects, the pixels of the overlapped regions are simply determined by their energy values in the corresponding Summation-FSP-FR energy maps.
It should be noted that the deconvolutional operation in a CNN model in back-propagation process will cause offset in the energy map, which leads to dilation around object edges.
Thus, we regularize the Summation-FSP-FR energy into the super-pixels generated by the method proposed in~\cite{dollar2015fast} to preserve objects' edge.

\begin{procedure}[!htb]
\caption{Experimental procedure of weakly supervised semantic segmentation}
\label{alg:Framwork}
\begin{algorithmic}[1]
\State Given a test image and a predicted label by the trained VggNet;
  \State Run FSP according to the predicted label and obtain Summation Energy Map;
  \State Select one of the middle-level layers (e.g., ``conv5\_2") as the target layer for FR, set preserved neurons as targets for FR algorithm, and run FR simultaneously to get the FSP-FR energy map;
  \State Add Summation Energy Map and FSP-FR energy map to obtain Summation-FSP-FR energy map;
  \State Get super-pixels by the algorithm proposed in ~\cite{dollar2015fast}, and let the energy value of each super-pixel be equal to the minimum Summation-FSP-FR energy value within that super-pixel;
 % \State Set pixels with energy value in Super-pixel energy map that lower than 20\% responses  as background, between 20\% and 70\%as probable background, between 70\% and 95\% as probable foreground, and higher than 95\% as foreground;
  \State Run the saliency cut to get the segmentation results.
\end{algorithmic}
\end{procedure}

\begin{figure}[!htb]
\vspace{0mm}
\centering
\includegraphics[height = 0.6\linewidth,width = 0.8\linewidth]{figure/experiments/segmentation/bad.pdf}
\caption{Failed examples. (a) Input images. (b) Ground truth segmentations. (c) Our segmentation results. (d)(e) Energy maps for different objects generated by our approach.}
\label{fig:badseg}
\end{figure}
The quantitative results on over-all validation set are listed in Table~\ref{table:weakly segmentation}.
We compare the performance of our weakly supervised approach with several state-of-the-art approaches with the same experimental setup, i.e. using only images from Pascal VOC2012 and only image-level labels.
The results reveal that our approach largely outperforms previous techniques.
Particularly, we achieve 10.76\% higher mIOU score than the state-of-the-art approaches and update the best records of 16 classes of Pascal VOC2012.
Fig.~\ref{fig:goodseg} illustrates some successful examples, where we find that even for very complex scenes, the proposed approach still works well.
Also we show some failure cases and their corresponding objects' energy maps in Fig.~\ref{fig:badseg}.
We observe that the energy maps are quite meaningful but the segmentation results are not satisfactory.
The possible reason derives from the saliency cut~\cite{saliencycut}, which implies that our approach still has the potential to be improved.
\section{Conclusion}
In this paper, we have proposed the feedback CNN, in which pruning and recovering operations are introduced to implement feedback in deep Convolutional Neural Networks.
Feedback CNN achieves the selectivity of neuron activations by jointly reasoning about the outputs of class nodes and the activations of hidden layer neurons.
It is capable of capturing high-level semantic concepts and projects information into image representation as energy maps, based on which recognition, localization and segmentation can be integrated into one unified framework.
Qualitative and quantitative experimental results on ImageNet2012 and Pascal VOC2012 have demonstrated the effectiveness of the proposed Feedback CNN.

Due to its importance in both human visual system and machine vision, the feedback mechanism deserves more attention.
It still has much space to be further explored, e.g., exploiting neurons in representing multiple objects with the same category, which is critical for instance segmentation.
We believe that, with more study of the feedback mechanism, it is highly possible to promote the development in the fields of pattern recognition and computer vision.

\bibliographystyle{IEEEtran}
\bibliography{pami_ccs}
\end{document}


